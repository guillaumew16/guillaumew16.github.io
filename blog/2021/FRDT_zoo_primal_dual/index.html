<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Regularized linear models and the Fenchel-Rockafellar duality theorem (II): A zoo of primal-dual methods | Guillaume Wang </title> <meta name="author" content="Guillaume Wang"> <meta name="description" content="Machine learning theory, applied mathematics, etc. "> <meta name="keywords" content="machine learning, mathematics, optimization, optimal transport"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?aa2fd88e52df6cb3146c60000125eab2"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://guillaumew16.github.io/blog/2021/FRDT_zoo_primal_dual/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Guillaume</span> Wang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Research blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <link rel="stylesheet" href="/assets/css/post_custom.css?5be8581675d5ca56cf9d2d53c35cbaf6"> <div class="post"> <header class="post-header"> <h1 class="post-title">Regularized linear models and the Fenchel-Rockafellar duality theorem (II): A zoo of primal-dual methods </h1> <p class="post-meta"> Created in September 03, 2021 by Guillaume Wang </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>This is the second of a series of posts on optimization of regularized linear models through the lens of duality. See the first one <a href="/blog/2021/FRDT_generalities">here</a>.</p> </blockquote> <ul id="markdown-toc"> <li> <a href="#saddle-point-formulation-mix-and-match-primal-and-dual-updates" id="markdown-toc-saddle-point-formulation-mix-and-match-primal-and-dual-updates">Saddle-point formulation: “mix-and-match” primal and dual updates</a> <ul> <li><a href="#examples" id="markdown-toc-examples">Examples</a></li> </ul> </li> <li> <a href="#duality-gap-formulation-and-fully-dual-approach-the-frank-wolfe-algorithm" id="markdown-toc-duality-gap-formulation-and-fully-dual-approach-the-frank-wolfe-algorithm">Duality-gap formulation and fully dual approach: the Frank-Wolfe algorithm</a> <ul> <li><a href="#a-trick-to-enforce-the-wrong-kkt-condition" id="markdown-toc-a-trick-to-enforce-the-wrong-kkt-condition">A trick to enforce the “wrong” KKT condition</a></li> <li><a href="#relation-to-frank-wolfe" id="markdown-toc-relation-to-frank-wolfe">Relation to Frank-Wolfe</a></li> <li><a href="#equivalence-to-a-fully-dual-approach" id="markdown-toc-equivalence-to-a-fully-dual-approach">Equivalence to a fully dual approach</a></li> </ul> </li> </ul> <p>We will continue with the notation from last time, in particular:</p> <ul> <li> <p>the primal problem is</p> \[\label{eq:FRDT_primal} \tag{P} \min_{w \in \mathcal{W}} \Psi(w) + \mathcal{L}(V w) =: P(w)\] </li> <li> <p>the dual problem is</p> \[\label{eq:FRDT_dual} \tag{D} \max_{a \in \mathcal{Y}^*} - \Psi^*(-V^* a) - \mathcal{L}^*(a) =: D(a).\] </li> <li> <p>the KKT conditions are</p> \[\label{eq:FRDT_KKT_Psi} \tag{$\mathrm{KKT}_\Psi$} w \in \partial \Psi^*(-V^* a) ~~\text{i.e}~~ -V^* a \in \partial \Psi(w)\] \[\label{eq:FRDT_KKT_L} \tag{$\mathrm{KKT}_{\mathcal{L}}$} a \in \partial \mathcal{L}(V w) ~~\text{i.e}~~ V w \in \partial \mathcal{L}^*(a)\] </li> </ul> <h2 id="saddle-point-formulation-mix-and-match-primal-and-dual-updates">Saddle-point formulation: “mix-and-match” primal and dual updates</h2> <p>The FRDT essentially tells us that the problem of fitting a regularized linear model to data, the minimization problem \(\eqref{eq:FRDT_primal}\), can be formulated as a saddle-point problem:</p> \[\min_{w \in \mathcal{W}} \max_{a \in \mathcal{Y}^*}~ F(w,a) =: \Psi(w) + \left\langle Vw, a \right\rangle - \mathcal{L}^*(a)\] <p>Let us naively think about how to iteratively solve that saddle-point problem; that is, how to choose an update rule for the joint variable \((w_t, a_t)\). I can think of four reasonable update rules for \(w_{t+1}\), given \((w_t,a_t)\):</p> <ul> <li> <p><strong>Fully optimize for fixed \(a\):</strong> By definition, for a fixed value of \(a\), the \(\mathop{\mathrm{arg\,min}}\) of the objective over \(w\) is computable in closed form as</p> \[\mathop{\mathrm{arg\,min}}_{w'} F(w',a) = \mathop{\mathrm{arg\,min}}_{w'} \Psi(w') + \left\langle w', V^* a \right\rangle = \mathop{\mathrm{arg\,max}}_{w'} \left\langle w', -V^* a \right\rangle - \Psi(w') = \partial \Psi^*(-V^* a).\] <p>So we may take as update rule:</p> \[w_{t+1} \in \partial \Psi^*(-V^* a_t)\] <p>(This can be interpreted as enforcing the condition \(\eqref{eq:FRDT_KKT_Psi}\) throughout the optimization procedure.) However, this update rule may not be computationally feasible.</p> </li> <li> <p><strong>Gradient descent step:</strong> Instead of fully optimizing, we may take one (or several) (sub)gradient descent step(s) for \(\mathop{\mathrm{arg\,min}}_{w'} F(w',a_t)\) starting from \(w_t\). This gives the update rule:</p> \[w_{t+1} \in w_t - \eta_t \partial_w F(w_t,a_t) = w_t - \eta_t \left[ \partial \Psi(w_t) + V^* a_t \right]\] </li> <li> <p><strong>Mirror descent step using \(\Psi\):</strong> Instead of gradient descent, we may take one step of the other basic optimization primitive that is mirror descent. A natural candidate for the link function is \(\Psi\) (assuming it is strictly convex and differentiable everywhere), yielding the update rule</p> \[\nabla \Psi(w_{t+1}) = \nabla \Psi(w_t) - \eta_t \left[ \nabla \Psi(w_t) + V^* a_t \right]\] </li> <li> <p><strong>Proximal gradient step:</strong> Instead of mirror descent, we may take one step of the third basic optimization primitive that is proximal gradient descent. It is arguably more natural than mirror descent since the objective if composite. This gives the update rule</p> \[w_{t+1} = \mathrm{prox}_{\eta_t \Psi} \left( w_t - \eta_t V^* a_t \right)\] </li> </ul> <p>As for update rules for \(a_{t+1}\) given \((w_t,a_t)\), the four same ideas apply.</p> <ul> <li> <p><strong>Fully optimize for fixed \(w\):</strong> \(a_{t+1} \in \partial \mathcal{L}(V w_t)\)</p> </li> <li> <p><strong>Gradient descent step:</strong> \(a_{t+1} \in a_t + \sigma_t \left[ - \partial \mathcal{L}^*(a_t) + V w_t \right]\)</p> </li> <li> <p><strong>Mirror descent step using \(\mathcal{L}^*\):</strong> \(\nabla \mathcal{L}^*(a_{t+1}) = \nabla \mathcal{L}^*(a_t) + \sigma_t \left[ - \nabla \mathcal{L}^*(a_t) + V w_t \right]\)</p> </li> <li> <p><strong>Proximal gradient step:</strong> \(a_{t+1} = \mathrm{prox}_{\sigma_t \mathcal{L}^*} \left( a_t + \sigma_t V w_t \right)\)</p> </li> </ul> <p>Thus, this naive reasoning gives a set of optimization schemes that one can try: simply mix-and-match the choice of update rules for \(w_{t+1}\) and for \(a_{t+1}\), and apply the updates alternatingly. By alternating updates we mean: \(w_{t+1} = \text{Update}_w(w_t,a_t)\), \(a_{t+1} = \text{Update}_a(w_{t+1},a_t)\). One can even consider using joint updates: \(w_{t+1} = \text{Update}_w(w_t,a_t)\), \(a_{t+1} = \text{Update}_a(w_t,a_t)\).</p> <h3 id="examples">Examples</h3> <ul> <li> <p><strong>Fully-optimizing in the dual recovers vanilla primal methods.</strong> Indeed if we substitute \(a_t\) by \(\partial \mathcal{L}(V w_t)\) in the primal update rules, then the term \(V^* a_t\) becomes</p> \[V^* \partial \mathcal{L}(V w_t) = \left. \partial_w \mathcal{L}(V w) \right|_{w_t}\] <p>the subgradient of the data-fitting term w.r.t the primal variable.</p> </li> <li> <p><strong>Proximal gradient steps in the dual.</strong> The algorithm consisting of alternating proximal gradient steps both for \(w_{t+1}\) and for \(a_{t+1}\), is called the Arrow-Hurwicz method. The well-known Chambolle-Pock algorithm can be interpreted as a fancier version of this scheme, whereby proximal gradient steps for \(a_{t+1}\) are alternated with a form of accelerated proximal gradient for \(w_{t+1}\): <a href="https://hal.archives-ouvertes.fr/hal-00490826/document" rel="external nofollow noopener" target="_blank">(Chambolle and Pock, 2011)</a></p> \[\begin{aligned} a_{t+1} &amp;= \mathrm{prox}_{\sigma \mathcal{L}^*} \left( a_t + \sigma V {\overline{w}}_t \right) \\ w_{t+1} &amp;= \mathrm{prox}_{\tau \Psi} \left( w_t - \tau V^* a_{t+1} \right) \\ {\overline{w}}_{t+1} &amp;= w_{t+1} + \theta (w_{t+1} - w_t)\end{aligned}\] <p>and the parameters \(\sigma, \tau, \theta\) can further be made to depend on \(t\).</p> <p>As a second example, the proximal dual coordinate ascent algorithm proposed in <a href="https://arxiv.org/abs/2003.13807" rel="external nofollow noopener" target="_blank">(Raj and Bach, 2021)</a>, and its accelerated variant, are also instances of the scheme explained above. There, the primal variables are fully optimized (\(w_t = \nabla \Psi^*(-V^* a_t)\)), and the dual variables are updated by a proximal gradient step. That paper focuses on the specific case of min-\(\Psi\)-interpolation, so \(\mathcal{L}^*\) consists in a sum of indicator functions, and they use an explicit expression for \(\mathrm{prox}_{\mathcal{L}^*}\).</p> </li> <li> <p><strong>Kernel methods (RKHS).</strong> For (Hilbert) kernel methods, \(\mathcal{W}\) is the RKHS and the regularizer is \(\Psi(w) = \frac{\lambda}{2} \left\lVert w \right\rVert^2\). So \(\partial \Psi^*(-V^* a_t) = -\lambda V^* a_t\), and one may fully-optimize in the primal and run e.g gradient descent entirely in the dual. In function space this corresponds to parametrizing the model as \(f = \sum_{i=1}^n a_i k(\cdot, x_i)\) and running gradient descent on the coefficients \(a_i\).</p> </li> </ul> <h2 id="duality-gap-formulation-and-fully-dual-approach-the-frank-wolfe-algorithm">Duality-gap formulation and fully dual approach: the Frank-Wolfe algorithm</h2> <p>For a given pair of variables \((w,a)\), we call <em>duality gap</em> the quantity \(P(w) - D(a)\). Since \(P(w) \geq P_{\text{opt}} \geq D_{\text{opt}} \geq D(a)\), the duality gap is non-negative, and provides an optimality certificate for the primal: \(P(w) - P_{\text{opt}} \leq P(W) - D(a)\). So instead of solving the saddle-point problem \(\min_w \max_a F(w,a)\), we may consider solving the duality-gap minimization problem</p> \[\min_{w \in \mathcal{W}} \min_{a \in \mathcal{Y}^*} P(w) - D(a).\] <p>Observe that the duality gap can be split into two terms as:</p> \[\begin{aligned} P(w) - D(a) &amp;= \left[ P(w) - F(w,a) \right] + \left[ F(w,a) - D(a) \right] \\ &amp;= \left[ \mathcal{L}^*(a) + \mathcal{L}(Vw) - \left\langle Vw, a \right\rangle \right] + \left[ \Psi^*(-V^*a) + \Psi(w) - \left\langle w, -V^* a \right\rangle \right].\end{aligned}\] <p>Both bracketed terms are non-negative. The first term is zero iff \(\eqref{eq:FRDT_KKT_L}\) is satisfied, and the second term is zero iff \(\eqref{eq:FRDT_KKT_Psi}\) is satisfied.</p> <p>The above suggests the following idea:</p> <ul> <li> <p>Choose an update rule for \(w_{t+1}\) such that \(\eqref{eq:FRDT_KKT_L}\) is always satisfied, so that at each step, \(F(w_t,a_t) = P(w_t)\);</p> </li> <li> <p>Choose an update rule for \(a_{t+1}\) that takes a step towards minimizing the second term in the duality gap: \(\min_{a'} F(w_t,a') - D(a') = \Psi^*(-V^*a') + \Psi(w_t) - \left\langle w_t, -V^* a' \right\rangle\).</p> </li> </ul> <p>Note that, compared to the saddle-point paradigm from the previous subsection, this idea seems completely backwards:</p> <ul> <li> <p>We saw that fully-optimizing in the primal for the saddle-point problem \(\min_w \max_a F(w,a)\) leads to choosing \(w_t\) that always satisfies the KKT condition for \(\Psi\) \(\eqref{eq:FRDT_KKT_Psi}\); whereas here we enforce the KKT condition for \(\mathcal{L}\) \(\eqref{eq:FRDT_KKT_L}\).</p> </li> <li> <p>For the dual update, the saddle-point approach suggests to choose \(a_{t+1}\) as taking a step towards \(\max_{a'} F(w_t,a')\), i.e to take a step towards minimizing the first term in the duality gap: \(\min_{a'} P(w_t) - F(w_t,a')\); whereas here we take a step towards minimizing the second term.</p> </li> </ul> <p>This is due to the fact that here we choose \(w_t\) to optimize only the first term of the duality-gap split: \(P(w) - F(w,a)\), and boldly ignored the second term…</p> <h3 id="a-trick-to-enforce-the-wrong-kkt-condition">A trick to enforce the “wrong” KKT condition</h3> <p>To actually implement the idea explained above, we are faced with a difficulty. The primal update rule is to choose \(w_t\) such that \(V w_t \in \partial \mathcal{L}^*(a_t)\). A naive approach is to dumbly compute \(\partial \mathcal{L}^*(a_t)\) and to somehow find a \(w_t\) in its preimage by \(V\). But \(V\), the evaluation operator, is typically difficult to invert (think of \(V\) as the data matrix and \(V \in \mathbb{R}^{n \times p}\) with \(p \gg n\)).</p> <p>Now comes a trick: suppose we have, at timestep \(t\), \(V w_t \in \partial \mathcal{L}^*(a_t)\), and we want to construct \(w_{t+1}\) such that \(V w_{t+1} \in \partial \mathcal{L}^*(a_{t+1})\). Further suppose that we chose to update \(a_{t+1}\) by mirror descent using \(\mathcal{L}^*\):</p> \[\begin{aligned} \nabla \mathcal{L}^*(a_{t+1}) &amp;= \nabla \mathcal{L}^*(a_t) - \sigma_t \left. \partial_a \left( \Psi^*(-V^*a) + \Psi(w_t) - \left\langle w_t, -V^* a \right\rangle \right) \right|_{a_t} \\ &amp;= \nabla \mathcal{L}^*(a_t) + \sigma_t V \nabla \Psi^*(-V^* a_t) - \sigma_t V w_t.\end{aligned}\] <p>Thus, we wish to construct \(w_{t+1}\) such that</p> \[V w_{t+1} = V w_t + \sigma_t V \nabla \Psi^*(-V^* a) - \sigma_t V w_t.\] <p>Clearly a possible choice is to simply set \(w_{t+1} = (1-\sigma_t) w_t + \sigma_t \nabla \Psi^*(-V^* a_t)\) !</p> <p>Thus, we obtain the following algorithm for implicit regularization:</p> \[\begin{aligned} w_0, a_0 &amp; ~\text{such that}~ a_0 \in \partial \mathcal{L}(Vw_0) \\ \nabla \mathcal{L}^*(a_{t+1}) &amp;= \nabla \mathcal{L}^*(a_t) + \sigma_t V \nabla \Psi^*(-V^* a_t) - \sigma_t V w_t \\ w_{t+1} &amp;= (1-\sigma_t) w_t + \sigma_t \nabla \Psi^*(-V^* a_t)\end{aligned}\] <p>which can be simplified as</p> \[\begin{aligned} w_0, a_0 &amp; ~\text{such that}~ a_0 \in \partial \mathcal{L}(Vw_0) \\ a_t &amp;= \nabla \mathcal{L}(V w_t) \\ w_{t+1} &amp;= (1-\sigma_t) w_t + \sigma_t \nabla \Psi^*(-V^* a_t)\end{aligned}\] <p>To avoid confusion, note that the first equation (defining \(a_{t+1}\)) is what we called the primal update rule; and that the second equation (which looks like a gradient step for \(w_{t+1}\)) is actually the mirror descent update for the dual.</p> <p>This trick that allows to choose \(w_{t+1}\) satisfying \(\eqref{eq:FRDT_KKT_L}\), can also be applied to other choices of the dual update. A crucial ingredient is that the dual update should involve mirror descent using \(\mathcal{L}^*\). In particular, the trick can be applied for accelerated mirror descent in the dual: see the paragraph just below Lemma 3.2 in <a href="http://arxiv.org/abs/2107.00595" rel="external nofollow noopener" target="_blank">(Ji, Srebro and Telgarsky, 2021)</a>, as well as their Appendix B.</p> <h3 id="relation-to-frank-wolfe">Relation to Frank-Wolfe</h3> <p>Note that the above method seems significantly different from anything we could arrive to by mixing-and-matching primal and dual updates for the saddle-point formulation. Indeed, the primal update rule</p> \[\begin{aligned} u_t &amp;= \partial \Psi^*(-V^*a_t) \\ w_{t+1} &amp;= (1-\sigma_t) w_t + \sigma_t u_t\end{aligned}\] <p>looks a bit mysterious: it’s neither a straightforward variant of gradient descent, nor of mirror descent, nor of proximal gradient descent.</p> <p>It’s strongly reminiscent, though, of the Frank-Wolfe a.k.a conditional gradient method,<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">1</a></sup> since the primal update consists in setting \(w_{t+1}\) to a convex combination of \(w_t\) and \(u_t\). And indeed, it may be seen as a generalization of Frank-Wolfe to regularized instead of constrained optimization problems (<a href="http://arxiv.org/abs/1211.6302" rel="external nofollow noopener" target="_blank">Bach 2013</a>, equation (17)). To see this, consider the case where \(\Psi(w) = \iota_\Omega(w)\) for some convex set \(\Omega\). Then, denoting \(g_t = V^* a_t\), the above method is equivalent to</p> \[\begin{aligned} g_t &amp;= V^* a_t = V^* \nabla \mathcal{L}(V w_t) = \left.\nabla_w \mathcal{L}(V w)\right|_{w_t} \\ u_t &amp;= \partial \Psi^*(-g_t) = \mathop{\mathrm{arg\,max}}_s \left\langle s, -g_t \right\rangle - \Psi(s) = \mathop{\mathrm{arg\,min}}_{s \in \Omega} \left\langle s, g_t \right\rangle \\ w_{t+1} &amp;= (1-\sigma_t) w_t + \sigma_t u_t\end{aligned}\] <p>which is exactly the Frank-Wolfe algorithm for the problem \(\min_w \mathcal{L}(Vw) ~\text{s.t}~ w \in \Omega\).</p> <h3 id="equivalence-to-a-fully-dual-approach">Equivalence to a fully dual approach</h3> <p>Denote \(S_P\) (resp. \(S_D\)) the optimal solution set for the primal problem \(\eqref{eq:FRDT_primal}\) (resp. dual problem \(\eqref{eq:FRDT_dual}\)). According to the FRDT, \(w \in S_P\) iff there exists \(a \in S_D\) such that \(\eqref{eq:FRDT_KKT_L}\). This motivates the following fully dual approach:</p> <ul> <li> <p>Choose an update rule for \(w_{t+1}\) such that \(\eqref{eq:FRDT_KKT_L}\) is always satisfied;</p> </li> <li> <p>Choose an update rule for \(a_{t+1}\) that takes a step towards solving the dual problem: \(\max_{a'} D(a')\) i.e \(\min_{a'} -D(a') = \Psi^*(-V^* a') + \mathcal{L}^*(a')\).</p> </li> </ul> <p>For the dual update rule we may choose mirror descent using \(\mathcal{L}^*\). For the primal update rule, we may apply the same trick as above to enforce \(\eqref{eq:FRDT_KKT_L}\) Thus we are led to the exact same algorithm as by the duality-gap approach.</p> <p>In hindsight this equivalence is not surprising at all. Indeed, since we enforce \(\eqref{eq:FRDT_KKT_L}\), the dual problem (towards solving which we let \(a_{t+1}\) take a step) is the same in both approaches:</p> \[\min_{a'} F(w_t,a')-D(a') = P(w_t)-D(a') \equiv \max_{a'} D(a').\] <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:3" role="doc-endnote"> <p><a href="https://en.wikipedia.org/wiki/Frank-Wolfe_algorithm" rel="external nofollow noopener" target="_blank">https://en.wikipedia.org/wiki/Frank-Wolfe_algorithm</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Guillaume Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: October 26, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1NEBMFFCE9"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1NEBMFFCE9");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>