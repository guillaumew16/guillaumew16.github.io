<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Regularized linear models and the Fenchel-Rockafellar duality theorem (I): Generalities | Guillaume Wang </title> <meta name="author" content="Guillaume Wang"> <meta name="description" content="Machine learning theory, applied mathematics, etc. "> <meta name="keywords" content="machine learning, mathematics, optimization, optimal transport"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?aa2fd88e52df6cb3146c60000125eab2"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://guillaumew16.github.io/blog/2021/FRDT_generalities/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Guillaume</span> Wang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Research blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <link rel="stylesheet" href="/assets/css/post_custom.css?5be8581675d5ca56cf9d2d53c35cbaf6"> <div class="post"> <header class="post-header"> <h1 class="post-title">Regularized linear models and the Fenchel-Rockafellar duality theorem (I): Generalities </h1> <p class="post-meta"> Created in August 27, 2021 by Guillaume Wang </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>This is the first of a series of posts on optimization of regularized linear models through the lens of duality. The series will be essentially a summary of what I learned during the first few weeks of my internship, when I looked into topics related to learning in Banach spaces (before I moved on to different, more concrete topics). The relevant convex analysis background can be found in last time’s <a href="/blog/2021/func_convex_analysis_cheatsheet">cheatsheet</a> – which is basically the zero-th post of this series.</p> <p>With the background definitions under our belt, we are <em>almost</em> ready to talk about concrete consequences of convex duality. As it turns out, to get a principled understanding of the many places where duality pops up, it is beneficial to first present the Fenchel-Rockafellar duality theorem, a kind of “master theorem” for convex duality.</p> </blockquote> <p>Many smart observations on (explicitly or implicitly) regularized linear models, as well as on convex optimization methods, make use of some sort of convex duality argument. For example, already in introductory ML courses, Lagrangian duality is typically used to show that the SVM solution is equivalent to the max-margin classifier. In this document we present a unified framework for these duality arguments, that makes it conceptually easier to draw connections.</p> <p>On a personal note, I have always found convex and Langrangian duality to be particularly black magic. The nice thing about the Fenchel-Rockafellar duality theorem is that it is general enough to contain all of that black magic, making duality-based derivations easier to follow.</p> <ul id="markdown-toc"> <li> <a href="#sec:setup_notation" id="markdown-toc-sec:setup_notation">Generic supervised learning setup and notation</a> <ul> <li><a href="#standard-notations-for-convex-duality" id="markdown-toc-standard-notations-for-convex-duality">Standard notations for convex duality.</a></li> </ul> </li> <li><a href="#sec:FRDT" id="markdown-toc-sec:FRDT">Fenchel-Rockafellar duality theorem (FRDT)</a></li> <li> <a href="#sec:variants_of_regu" id="markdown-toc-sec:variants_of_regu">Variants of regularization: penalized, constrained, min-norm interpolation</a> <ul> <li><a href="#coming-up-next" id="markdown-toc-coming-up-next">Coming up next…</a></li> </ul> </li> </ul> <h3 id="sec:setup_notation">Generic supervised learning setup and notation</h3> <p>Consider supervised learning with a linear model (linear in the parameters) i.e a hypothesis space of the form</p> \[\mathcal{F}= \left\lbrace f_w: x \mapsto \left\langle w, \phi(x) \right\rangle, w \in \mathcal{W} \right\rbrace\] <p>for some feature map \(\phi: \mathcal{X}\to \mathcal{W}^*\), where \(\mathcal{X}\) is input space and \(\mathcal{W}\) is a Banach parameter space.</p> <p>Suppose we are given a dataset \((x_i, y^{\text{tgt}}_i)_{i \leq n}\) and we want to solve an optimization problem of the form <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p> \[\mathop{\mathrm{arg\,min}}_{w \in \mathcal{W}} \Psi(w) + \frac{1}{n} \sum_{i=1}^n \ell \left( y^{\text{tgt}}_i, \left\langle w, \phi(x_i) \right\rangle \right),\] <p>where each \(\ell(y^{\text{tgt}}_i, \cdot)\) is a convex function, and the regularizer term \(\Psi\) is convex. This generic formulation captures pretty much all standard supervised learning settings, see the last section of this post.</p> <p>Further pose the shorthands:</p> <ul> <li> <p>\(\mathcal{Y}= (\mathbb{R}^n, \left\lVert \cdot \right\rVert)\) for some arbitrary norm; \(\mathcal{Y}^*\) will denote \(\mathbb{R}^n\) equipped with the dual norm \(\left\lVert \cdot \right\rVert_*\);</p> </li> <li> <p>\(\mathcal{L}(y) = \frac{1}{n} \sum_i \ell(y^{\text{tgt}}_i, y_i)\) the data-fitting term;</p> </li> <li> <p>\(V: \left[ \mathcal{W}\to \mathcal{Y}, w \mapsto (\left\langle w, \phi(x_i) \right\rangle)_{i \leq n} \right]\) the evaluation operator.</p> </li> </ul> <h4 id="standard-notations-for-convex-duality">Standard notations for convex duality.</h4> <ul> <li> <p>For a Banach space \(E\), the dual space is denoted \(E^*\).</p> </li> <li> <p>The set of proper, lower-semicontinuous (l.s.c), and convex functions over \(E\) is denoted \(\Gamma(E)\).</p> </li> <li> <p>For a convex function \(\Psi\), \(\Psi^*\) denotes the convex conjugate.</p> </li> <li> <p>For a linear operator \(V: \mathcal{W}\to \mathcal{Y}\) between Banach spaces, \(V^*: \mathcal{Y}^* \to \mathcal{W}^*\) denotes the adjoint operator between the dual spaces.</p> </li> <li> <p>The indicator function of a set \(A\) is defined as \(\iota_A(x) = \begin{cases} \infty \text{ if } x \not\in A \\ 0 \text{ if } x \in A \end{cases}\).</p> </li> <li> <p>\(\left\lVert \cdot \right\rVert\) denotes an arbitrary norm, and \(\left\lVert \cdot \right\rVert_*\) denotes its dual norm.</p> </li> </ul> <h3 id="sec:FRDT">Fenchel-Rockafellar duality theorem (FRDT)</h3> <p>Let us state the FRDT with machine-learning-friendly notation, as motivated above.</p> <div class="theorem" text="FRDT"> <p>Let \(\mathcal{W}\) and \(\mathcal{Y}\) be two real Banach spaces. Let \(\Psi \in \Gamma(\mathcal{W})\), let \(\mathcal{L}\in \Gamma(\mathcal{Y})\), and let \(V: \mathcal{W}\to \mathcal{Y}\) be a bounded linear operator. Consider the primal problem</p> \[\label{eq:FRDT_primal} \tag{P} \min_{w \in \mathcal{W}} \Psi(w) + \mathcal{L}(V w) =: P(w)\] <p>and define the dual problem as</p> \[\label{eq:FRDT_dual} \tag{D} \max_{a \in \mathcal{Y}^*} - \Psi^*(-V^* a) - \mathcal{L}^*(a) =: D(a).\] <p>Denote \(S_P\) and \(S_D\) their respective optimal solution sets. Denote the KKT conditions</p> \[\label{eq:FRDT_KKT_Psi} \tag{$\mathrm{KKT}_\Psi$} w \in \partial \Psi^*(-V^* a) ~~\text{i.e}~~ -V^* a \in \partial \Psi(w)\] \[\label{eq:FRDT_KKT_L} \tag{$\mathrm{KKT}_{\mathcal{L}}$} a \in \partial \mathcal{L}(V w) ~~\text{i.e}~~ V w \in \partial \mathcal{L}^*(a)\] <p>Weak duality holds: for all \(w\) and \(a\), \(P(w) \geq P_{\text{opt}} \geq D_{\text{opt}} \geq D(a)\).</p> <p>Suppose that \(0 \in \text{interior}( V(\mathop{\mathrm{dom}}\Psi)- \mathop{\mathrm{dom}}\mathcal{L})\). Then strong duality holds:</p> <ul> <li> <p>\(\eqref{eq:FRDT_primal}\) and \(\eqref{eq:FRDT_dual}\) have the same optimal value \(P_{\text{opt}} = D_{\text{opt}}\);</p> </li> <li> <p>\(w \in S_P\) and \(a \in S_D\) iff \(\eqref{eq:FRDT_KKT_L}\) and \(\eqref{eq:FRDT_KKT_Psi}\);</p> </li> <li> <p>\(w \in S_P\) iff there exists \(a \in \mathcal{Y}^*\) such that \(\eqref{eq:FRDT_KKT_Psi}\) and \(\eqref{eq:FRDT_KKT_L}\), iff there exists \(a \in S_D\) such that \(\eqref{eq:FRDT_KKT_L}\).</p> </li> </ul> </div> <div class="proof" text="sketched"> <p>The actual proof is much more complicated, <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> but hinges on the following calculation which is enough to gain intuition:</p> \[\begin{aligned} \eqref{eq:FRDT_primal} &amp;\equiv \min_w \Psi(w) + \mathcal{L}(Vw) \\ &amp;\equiv \min_w \max_a \Psi(w) + \left\langle Vw, a \right\rangle_\mathcal{Y}- \mathcal{L}^*(a) \\ &amp;\equiv \min_w \max_a \Psi(w) + \left\langle w, V^* a \right\rangle_\mathcal{W}- \mathcal{L}^*(a) \\ &amp;\geq \max_a \min_w - \left[ \left\langle w, -V^* a \right\rangle_\mathcal{Y}- \Psi(w) \right] - \mathcal{L}^*(a) \\ &amp;\equiv -\Psi^*(-V^*a) - \mathcal{L}^*(a) \equiv \eqref{eq:FRDT_dual} \end{aligned}\] <p>Denote \(F(w,a) = \Psi(w) + \left\langle Vw, a \right\rangle - \mathcal{L}^*(a)\). The above calculation shows that \(P(w) \geq F(w,a) \geq D(a)\) for all \(w,a\). To see where the KKT conditions come from, note that \(P(w) = D(a)\) implies</p> <ul> <li> <p>\(F(w,a) = P(w)\) i.e \(\left\langle Vw, a \right\rangle_\mathcal{Y}- \mathcal{L}^*(a)= \mathcal{L}(Vw)\), i.e \(a\) saturates the Fenchel-Young inequality, i.e \(a \in \partial \mathcal{L}(Vw)\);</p> </li> <li> <p>\(F(w,a) = D(a)\) i.e \(\left\langle w, -V^* a \right\rangle_\mathcal{Y}- \Psi(w) = \Psi^*(-V^*a)\), i.e \(w\) saturates the Fenchel-Young inequality, i.e \(w \in \partial \Psi^*(-V^* a)\).</p> </li> </ul> </div> <div class="remark"> <p>The condition that \(0 \in \text{interior}( V(\mathop{\mathrm{dom}}\Psi)- \mathop{\mathrm{dom}}\mathcal{L})\) is morally just a variant of Slater’s condition: “there exists a strictly feasible point”. There are other constraint qualification conditions that imply strong duality.</p> </div> <div class="remark"> <p>The adjoint of the evaluation operator, \(V^*\), is simply given by</p> \[\forall a \in \mathcal{Y}^* = \mathbb{R}^n,~ V^* a = \sum_{i=1}^n a_i \left\langle \cdot, \phi(x_i) \right\rangle = \left\langle \cdot, \sum_{i=1}^n a_i \phi(x_i) \right\rangle.\] <p>For finite-dimensional features, say \(\dim(\mathcal{W}) = p\), \(V\) can be seen as the transformed data matrix \(\begin{bmatrix} \phi(x_1) &amp; ... &amp; \phi(x_n) \end{bmatrix}^\top \in \mathbb{R}^{n \times p}\), and \(V^*\) is simply its transpose.</p> </div> <h3 id="sec:variants_of_regu">Variants of regularization: penalized, constrained, min-norm interpolation</h3> <p>It is common wisdom that <em>models with lower “complexity” have better generalization properties</em>.</p> <p>Traditionally, low “complexity” of the learned model is ensured in practice by adding a penalty term to the loss function. In our notation: \(f_w\) is chosen as minimizing \(\lambda \psi(w) + \mathcal{L}(Vw)\), for some regularizer or “complexity measure” \(\psi\). Still traditionally, a theory-friendlier alternative is to constrain the parameters to a low-complexity set \(\Omega\), e.g \(\Omega = \left\lbrace w; \psi(w) \leq B \right\rbrace\).</p> <p>Parallel to these ideas, also of interest for theory and practice are so-called <em>overparametrized</em> settings, whereby \(\dim(\mathcal{W}) \gg \dim(\mathcal{X})\), so that there exist many values of \(w\) that interpolate the data i.e such that \(\mathcal{L}(V w) = 0\). In such settings, the aforementioned common wisdom can be interpreted in a third way: select, among all interpolating values of \(w\), the one with the lowest “complexity”: \(\mathop{\mathrm{arg\,min}}_w \psi(w) ~\text{s.t}~ \mathcal{L}(V w) = 0\). <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></p> <p>Note that, since FRDT strictly generalizes Lagrangian duality, it offers a unified framework for:</p> <ul> <li> <p>regularization via penalization, by letting \(\Psi(w)\) be the penalty term \(\lambda \psi(w)\);</p> </li> <li> <p>regularization via constraining the parameters to a convex set \(\Omega\), by letting \(\Psi(w) = \iota_\Omega(w)\);</p> </li> <li> <p>min-regularizer e.g min-norm interpolation, by letting \(\mathcal{L}(Vw) = \iota_{ \{y^{\text{tgt}}\} }(Vw)\).</p> </li> </ul> <p>Regardless of the setting (penalized, constrained, or interpolation), many smart observations on linear models can be made by using some sort of duality argument. The nice thing about the FRDT is that it provides a unified way to formulate all those duality-based arguments.</p> <h4 id="coming-up-next">Coming up next…</h4> <p>In the next few posts, I will discuss a few example topics where convex duality arguments are invoked, through the lens of the FRDT. I hope to convince you that adopting that viewpoint is indeed very helpful for getting a principled understanding of those topics. Up next: a zoo of primal-dual methods for optimization of regularized linear models.</p> <hr> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Usually the dataset is denoted \((x_i, y_i)_{i \leq n}\) and the predictions are denoted \(\hat{y}_i\). Here we chose to denote \(y^{\text{tgt}}_i\) the target labels because it will be more convenient to denote simply \(y_i\) the predictions. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>The proof can be found in Section 31 of Rockafellar’s 1970 book “Convex analysis”, and on this webpage: <a href="https://pwacker.com/fenchelrockafellar.html" rel="external nofollow noopener" target="_blank">https://pwacker.com/fenchelrockafellar.html</a>, which also contains nice illustrative drawings. Apparently this blog post series was also planning to nicely present the proof: <a href="https://dohmatob.github.io/research/2019/10/31/duality.html" rel="external nofollow noopener" target="_blank">https://dohmatob.github.io/research/2019/10/31/duality.html</a>, but it hasn’t been updated in a while. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:3" role="doc-endnote"> <p>Note that this formalization of "data-interpolation" is only for regression, since for classification with the logistic loss for example, \(\mathcal{L}(V w) = 0\) is impossible. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Guillaume Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: October 26, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1NEBMFFCE9"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1NEBMFFCE9");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>